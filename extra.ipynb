{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking dataset number of frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "from decord import cpu, gpu\n",
    "from decord import VideoReader\n",
    "from tqdm import tqdm \n",
    "\n",
    "processed_dataset_path = \"datasets/VI_dataset_mix_224_10s/\"\n",
    "# processed_dataset_path = \"datasets/MIX_DATA/\"\n",
    "vid_paths = os.listdir(processed_dataset_path)\n",
    "print(len(vid_paths))\n",
    "frames = 0\n",
    "for vid_path in tqdm(vid_paths):\n",
    "    if vid_path[-1]=='t':\n",
    "        print(vid_path)\n",
    "        continue\n",
    "    vid_path = os.path.join(processed_dataset_path, vid_path)\n",
    "    # print(vid_path)\n",
    "    vr = VideoReader(vid_path, ctx=cpu(0))\n",
    "    frames += len(vr)\n",
    "    \n",
    "print(\"Total frames =\", frames)\n",
    "print(\"Total time =\", frames/(30*3600))\n",
    "\n",
    "# MIX_DATA - \n",
    "# Total long videos = 18 \n",
    "# Total frames = 1603628\n",
    "# Total time = 14.85 hour\n",
    "# MIX_Dataset -\n",
    "# Total short videos = 5269\n",
    "# Each of 10 sec - 30 fps\n",
    "# Total frames = 1580700\n",
    "# 14.64 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset_path = \"datasets/VI_dataset_mix_224_10s/\"\n",
    "split_file_path = os.path.join(processed_dataset_path, \"splits.csv\")\n",
    "vid_paths = os.listdir(processed_dataset_path)\n",
    "print(len(vid_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(vid_paths)*0.8)\n",
    "print(train_size, len(vid_paths)-train_size)\n",
    "random.shuffle(vid_paths)\n",
    "    \n",
    "with open(split_file_path, 'w', newline='')as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow(['name', 'split'])\n",
    "    for i, v in enumerate(vid_paths):\n",
    "        if i < train_size:\n",
    "            csvwriter.writerow([v, 'train'])\n",
    "        else:\n",
    "            csvwriter.writerow([v, 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def create_edge_map(image_tensor):\n",
    "    # image_np = image_tensor.transpose(1, 2, 0) * 255  # Convert to numpy format\n",
    "    image_gray = cv2.cvtColor(image_tensor.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Apply Sobel filter to get edges\n",
    "    edge_x = cv2.Sobel(image_gray, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    edge_y = cv2.Sobel(image_gray, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    edge_map = np.sqrt(edge_x**2 + edge_y**2)\n",
    "    # edge_map = torch.tensor(edge_map / edge_map.max(), dtype=torch.float32)\n",
    "    edge_map = edge_map / edge_map.max()\n",
    "    \n",
    "    return edge_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./panda.png')\n",
    "# print(img.shape)\n",
    "plt.imshow(img)\n",
    "plt.imshow(create_edge_map(img), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([img, img])#.rehsape()\n",
    "# b = np.concatenate([img,img], )\n",
    "print(b.shape, img.shape)\n",
    "\n",
    "image_gray = cv2.cvtColor(b.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Grayscale, ToPILImage, ToTensor\n",
    "\n",
    "pil = ToPILImage()\n",
    "gs = Grayscale()\n",
    "tensor = ToTensor()\n",
    "def detect_edges(img):\n",
    "    # Convert the image to grayscale (assumes img is a torch tensor with shape [C, H, W])\n",
    "    # gray_img = img.mean(dim=0, keepdim=True)  # Convert to single-channel grayscale\n",
    "    # gray_img = tensor(gs(pil(img)))\n",
    "    gray_img = gs(img)\n",
    "    print(gray_img.shape)\n",
    "    # Apply Sobel filter for edge detection in x and y directions\n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "    \n",
    "    edge_x = F.conv2d(gray_img, sobel_x, padding=1)\n",
    "    edge_y = F.conv2d(gray_img, sobel_y, padding=1)\n",
    "    \n",
    "    # Combine edges and normalize\n",
    "    edge_magnitude = torch.sqrt(edge_x**2 + edge_y**2).squeeze(0)\n",
    "    edge_mask = torch.clamp(edge_magnitude, 0, 1)  # Ensure values are in range [0, 1]\n",
    "    \n",
    "    return edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_img = torch.tensor(np.array([img, img])).permute(0, 3, 1, 2)\n",
    "# t_img.shape\n",
    "detect_edges(t_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
