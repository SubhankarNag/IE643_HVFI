{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ffmpeg-python, !pip uninstall gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.RIFE import Model\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import argparse\n",
    "from torch.nn import functional as F\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "# NOTE: Change the path accordingly\n",
    "model.flownet.load_state_dict(torch.load(os.path.join('./new_train_log', 'flownet.pkl'), map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "def save_video(frames, video_path):\n",
    "    print(frames[0].shape)\n",
    "    FPS, W, H = 30, frames[0].shape[1], frames[0].shape[0] \n",
    "    out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"xvid\"), FPS, (W, H))\n",
    "\n",
    "    for frame in frames:\n",
    "        # print(frame.shape)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def generate_intermidiate_frames(img0, img1, exp_num_frames):\n",
    "    img0 = (torch.tensor(img0.transpose(2, 0, 1)) / 255.).unsqueeze(0)\n",
    "    img1 = (torch.tensor(img1.transpose(2, 0, 1)) / 255.).unsqueeze(0)\n",
    "    \n",
    "    # padding\n",
    "    n, c, h, w = img0.shape\n",
    "    ph = ((h - 1) // 32 + 1) * 32\n",
    "    pw = ((w - 1) // 32 + 1) * 32\n",
    "    padding = (0, pw - w, 0, ph - h)\n",
    "    img0 = F.pad(img0, padding)\n",
    "    img1 = F.pad(img1, padding)\n",
    "    \n",
    "    # print(img0.shape, img1.shape)\n",
    "    \n",
    "    \n",
    "    # generating \n",
    "    img_list = [img0, img1]\n",
    "    for i in range(exp_num_frames):\n",
    "        tmp = []\n",
    "        for j in range(len(img_list) - 1):\n",
    "            mid = model.inference(img_list[j], img_list[j + 1])\n",
    "            tmp.append(img_list[j])\n",
    "            tmp.append(mid)\n",
    "        tmp.append(img1)\n",
    "        img_list = tmp\n",
    "        \n",
    "    # print(len(img_list))\n",
    "\n",
    "    # convert to pil\n",
    "    final_images = []\n",
    "    for img in img_list:\n",
    "        final_images.append(Image.fromarray(np.uint8(img.squeeze().permute(1, 2, 0).detach()* 255)).convert('RGB'))\n",
    "    \n",
    "    return final_images\n",
    "            \n",
    "\n",
    "def inference(type, data, exp_num_frames):\n",
    "    if type == 0:\n",
    "        frames = extract_frames(data)\n",
    "        print(len(frames))\n",
    "        frame_list = []\n",
    "        for i in tqdm(range(len(frames)-1)):\n",
    "            tmp_list = generate_intermidiate_frames(frames[i], frames[i+1], exp_num_frames)\n",
    "            frame_list += tmp_list[:-1]\n",
    "        frame_list += [Image.fromarray(frames[-1])]\n",
    "        print(len(frame_list))\n",
    "\n",
    "        video_path = os.path.splitext(data)[0] + \"_processed.avi\"  \n",
    "        save_video([np.array(img) for img in frame_list], video_path)  \n",
    "        print(video_path)\n",
    "        \n",
    "        return convert_to_mp4(video_path), frame_list\n",
    "        # return video_path ,frame_list\n",
    "    if type == 1: # img\n",
    "        print(\"IMAGE\")\n",
    "        img0, img1  = data\n",
    "        img0 = np.array(img0)\n",
    "        img1 = np.array(img1)\n",
    "        \n",
    "        return generate_intermidiate_frames(img0, img1, exp_num_frames)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mp4(video_path):\n",
    "    output_path = os.path.splitext(video_path)[0] + \"_converted.mp4\"\n",
    "    try:\n",
    "        ffmpeg.input(video_path).output(output_path, vcodec='libx264', acodec='aac').run(overwrite_output=True)\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting video: {e}\")\n",
    "        return video_path \n",
    "    \n",
    "def process_video(video, exp_num_frames):\n",
    "    video_path, frame_list = inference(0, video, exp_num_frames)\n",
    "    return video_path, frame_list # actually we need to return video path after processing\n",
    "\n",
    "# Extract frames from video for gallery display\n",
    "def extract_frames(video_path):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def process_images(img1, img2, exp_num_frames):\n",
    "    # print(\"IMAGE\")\n",
    "    return inference(1, [img1, img2], exp_num_frames)\n",
    "    # return \n",
    "\n",
    "# immediate conversion\n",
    "def handle_video_upload(video):\n",
    "    converted_video_path = convert_to_mp4(video)\n",
    "    return converted_video_path\n",
    "\n",
    "def interpolate(choice, video=None, img1=None, img2=None, exp_num_frames=None):\n",
    "    if choice == \"Upload a video\":\n",
    "        processed_video, frames = process_video(video, exp_num_frames)\n",
    "        return processed_video, None, frames  # Video output, no images, and frames gallery\n",
    "    elif choice == \"Upload two images\":\n",
    "        if img1 is not None and img2 is not None:\n",
    "            generated_frames = process_images(img1, img2, exp_num_frames)  # Placeholder for interpolated frames\n",
    "            return None, generated_frames, None  # No video, only images, no frames gallery\n",
    "        else:\n",
    "            return None, None, None\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h1 style='text-align: center;'>Video and Image Interpolation</h1>\")\n",
    "    choice = gr.Radio([\"Upload a video\", \"Upload two images\"], label=\"Choose Input Type\")\n",
    "\n",
    "\n",
    "    num_frames = gr.Slider(1, 10, value=2, step=1, label=\"Number of Interpolated Frames (2^n-1)\")\n",
    "\n",
    "    # Video input and processing\n",
    "    with gr.Row() as video_section:\n",
    "        video_input = gr.Video(label=\"Upload Video\", interactive=True)\n",
    "        video_output = gr.Video(label=\"Processed Video\", interactive=True)\n",
    "        frame_gallery = gr.Gallery(label=\"Processed Video Frames\")\n",
    "\n",
    "        \n",
    "    # Image input and processing\n",
    "    with gr.Row() as image_section:\n",
    "        img1_input = gr.Image(label=\"Upload First Image\", type=\"pil\")\n",
    "        img2_input = gr.Image(label=\"Upload Second Image\", type=\"pil\")\n",
    "        # num_frames = gr.Slider(1, 5, value=2, step=1, label=\"Number of Interpolated Frames\")\n",
    "        image_output = gr.Gallery(label=\"Generated Frames\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Initially hide both sections\n",
    "    video_section.visible = False\n",
    "    image_section.visible = False\n",
    "    \n",
    "     # Toggle input visibility based on choice\n",
    "    def toggle_inputs(choice):\n",
    "        if choice == \"Upload a video\":\n",
    "            return gr.update(visible=True), gr.update(visible=False), gr.update(visible=True), gr.update(visible=True)\n",
    "        elif choice == \"Upload two images\":\n",
    "            return gr.update(visible=False), gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
    "    \n",
    "    choice.change(toggle_inputs, inputs=[choice], outputs=[video_section, image_section, video_output, frame_gallery])\n",
    "\n",
    "    # Handle video upload to convert and display\n",
    "    video_input.upload(convert_to_mp4, inputs=video_input, outputs=video_input)\n",
    "\n",
    "    # Processing button\n",
    "    process_button = gr.Button(\"Process\")\n",
    "\n",
    "    # Ensure `interpolate` always returns outputs for video, gallery, and frames\n",
    "    process_button.click(\n",
    "        interpolate,\n",
    "        inputs=[choice, video_input, img1_input, img2_input, num_frames],\n",
    "        outputs=[video_output, image_output, frame_gallery]\n",
    "    )\n",
    "\n",
    "    gr.Markdown(\"### Instructions\")\n",
    "    gr.Markdown(\"\"\"\n",
    "        1. **Select** \"Upload a video\" to process an entire video offline or \"Upload two images\" to interpolate images.\n",
    "        2. **Adjust Settings** (e.g., number of frames) if available.\n",
    "        3. **Click Process** to view the interpolated video or frames. \n",
    "        \"\"\")\n",
    "    \n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
