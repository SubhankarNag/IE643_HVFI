{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ffmpeg-python, !pip uninstall gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import argparse\n",
    "from torch.nn import functional as F\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the model type to context based models\n",
    "from choose_model import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model()\n",
    "# NOTE: Change the path accordingly\n",
    "model.flownet.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 0.8\n",
    "thickness = 2\n",
    "\n",
    "def save_video(frames, video_path):\n",
    "    print(frames[0].shape)\n",
    "    FPS, W, H = 30, frames[0].shape[1], frames[0].shape[0] \n",
    "    out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*\"xvid\"), FPS, (W, H))\n",
    "\n",
    "    for frame in frames:\n",
    "        # print(frame.shape)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def write_text(image, is_gen=True):\n",
    "    org = (15, image.shape[0]-15)\n",
    "\n",
    "    color = (255, 41, 41) if is_gen else (21, 245, 186)\n",
    "    text = \"GENERATED\" if is_gen else \"ORIGINAL\"\n",
    "    image = cv2.putText(image.copy(), text, org, font, \n",
    "                    fontScale, color, thickness, cv2.LINE_AA)\n",
    "    return image\n",
    "    \n",
    "\n",
    "def generate_intermidiate_frames(img0_0, img0_1, img1_0, img1_1, exp_num_frames):\n",
    "    img0_0 = (torch.tensor(img0_0.transpose(2, 0, 1)) / 255.).unsqueeze(0) # 1, 3, 224, 224\n",
    "    img0_1 = (torch.tensor(img0_1.transpose(2, 0, 1)) / 255.).unsqueeze(0) # 1, 3, 224, 224\n",
    "    img1_0 = (torch.tensor(img1_0.transpose(2, 0, 1)) / 255.).unsqueeze(0)\n",
    "    img1_1 = (torch.tensor(img1_1.transpose(2, 0, 1)) / 255.).unsqueeze(0)\n",
    "    \n",
    "    # padding\n",
    "    n, c, h, w = img0_0.shape\n",
    "    ph = ((h - 1) // 32 + 1) * 32\n",
    "    pw = ((w - 1) // 32 + 1) * 32\n",
    "    padding = (0, pw - w, 0, ph - h)\n",
    "    img0_0 = F.pad(img0_0, padding)\n",
    "    img0_1 = F.pad(img0_1, padding)\n",
    "    img1_0 = F.pad(img1_0, padding)\n",
    "    img1_1 = F.pad(img1_1, padding)\n",
    "    \n",
    "    # print(img0.shape, img1.shape)\n",
    "    \n",
    "    \n",
    "    # generating \n",
    "    img_list = [img0_0, img0_1, img1_0, img1_1]\n",
    "    for i in range(exp_num_frames):\n",
    "        tmp = []\n",
    "        for j in range(0, len(img_list)-3, 1):\n",
    "            mid = model.inference(torch.concat((img_list[j], img_list[j+1]),1), torch.concat((img_list[j+2], img_list[j+3]),1))\n",
    "            # tmp.append(img_list[j])\n",
    "            # tmp.append(img_list[j+1])\n",
    "            tmp.append(mid)\n",
    "        \n",
    "        c = 0\n",
    "        for idx in range(2, len(img_list)-1, 1):\n",
    "            img_list.insert(idx+c, tmp[c])\n",
    "            c+=1\n",
    "            \n",
    "        # tmp.append(img1_0)\n",
    "        # tmp.append(img1_1)\n",
    "        # img_list = tmp\n",
    "        \n",
    "    # print(len(img_list))\n",
    "    c_img = -1\n",
    "    num_gen_frame = (2**exp_num_frames)-1\n",
    "    for idx, img in enumerate(img_list):\n",
    "        img = np.uint8(img.squeeze().permute(1, 2, 0).detach()* 255)\n",
    "        if c_img == 0 or c_img == -1: # 1st frame\n",
    "           img = write_text(img, is_gen=False)\n",
    "           c_img += 1\n",
    "        elif c_img == num_gen_frame+1: #2nd last frame\n",
    "           img = write_text(img, is_gen=False)\n",
    "           c_img +=1\n",
    "        elif c_img == num_gen_frame+2:\n",
    "            img = write_text(img, is_gen=False)\n",
    "            c_img = -1\n",
    "        else:\n",
    "           img = write_text(img, is_gen=True)\n",
    "           c_img += 1\n",
    "        img_list[idx] = img\n",
    "             \n",
    "    \n",
    "\n",
    "    # convert to pil\n",
    "    final_images = []\n",
    "    for img in img_list:\n",
    "        final_images.append(Image.fromarray(np.uint8(img)).convert('RGB'))\n",
    "    \n",
    "    return final_images\n",
    "\n",
    "def compute_avg_l1_diff(images):\n",
    "    frames = [np.array(img) for img in images]\n",
    "    \n",
    "    total_l1_loss = 0.0\n",
    "    num_pairs = len(frames) - 1  # Number of adjacent pairs\n",
    "    \n",
    "    for i in range(num_pairs):\n",
    "        l1_loss = np.mean(np.abs(frames[i] - frames[i + 1]))\n",
    "        total_l1_loss += l1_loss\n",
    "    \n",
    "    average_l1_loss = total_l1_loss / num_pairs if num_pairs > 0 else 0.0\n",
    "    \n",
    "    return average_l1_loss\n",
    "\n",
    "\n",
    "def inference(type, data, exp_num_frames):\n",
    "    if type == 0:\n",
    "        frames = extract_frames(data)\n",
    "        print(len(frames))\n",
    "        frame_list = []\n",
    "        print('Starting')\n",
    "        for i in tqdm(range(0, len(frames)-3, 2), total=(len(frames)-3)//2):\n",
    "            tmp_list = generate_intermidiate_frames(frames[i], frames[i+1], frames[i+2], frames[i+3], exp_num_frames)\n",
    "            frame_list += tmp_list[:-2]\n",
    "        # print(tmp_list[-1].shape)\n",
    "        frame_list += [tmp_list[-2]]\n",
    "        frame_list += [tmp_list[-1]]\n",
    "        print(len(frame_list))\n",
    "\n",
    "        video_path = os.path.splitext(data)[0] + \"_processed.avi\"  \n",
    "        save_video([np.array(img) for img in frame_list], video_path)  \n",
    "        print(video_path)\n",
    "        \n",
    "        average_l1_loss = compute_avg_l1_diff(frame_list)\n",
    "        print(\"Avg l1 difference =\", average_l1_loss)\n",
    "        \n",
    "        return convert_to_mp4(video_path), frame_list, average_l1_loss\n",
    "        # return video_path ,frame_list\n",
    "    if type == 1: # img\n",
    "        print(\"IMAGE\")\n",
    "        img0_0, img0_1, img1_0, img1_1  = data\n",
    "        img0_0, img0_1, img1_0, img1_1 = np.array(img0_0), np.array(img0_1), np.array(img1_0), np.array(img1_1)\n",
    "        \n",
    "        frame_list  = generate_intermidiate_frames(img0_0, img0_1, img1_0, img1_1, exp_num_frames)\n",
    "        \n",
    "        average_l1_loss = compute_avg_l1_diff(frame_list)\n",
    "        print(\"Avg l1 difference =\", average_l1_loss)\n",
    "        \n",
    "        \n",
    "        return frame_list, average_l1_loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def convert_to_mp4(video_path):\n",
    "    output_path = os.path.splitext(video_path)[0] + \"_converted.mp4\"\n",
    "    try:\n",
    "        ffmpeg.input(video_path).output(output_path, vcodec='libx264', acodec='aac').run(overwrite_output=True)\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting video: {e}\")\n",
    "        return video_path \n",
    "    \n",
    "def process_video(video, exp_num_frames):\n",
    "    video_path, frame_list, average_l1_loss = inference(0, video, exp_num_frames)\n",
    "    return video_path, frame_list, average_l1_loss # actually we need to return video path after processing\n",
    "\n",
    "# Extract frames from video for gallery display\n",
    "def extract_frames(video_path):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def process_images(img1, img1_1, img2, img2_1, exp_num_frames):\n",
    "    # print(\"IMAGE\")average_l1_loss\n",
    "    return inference(1, [img1, img1_1, img2, img2_1], exp_num_frames)\n",
    "    # return \n",
    "\n",
    "# immediate conversion\n",
    "def handle_video_upload(video):\n",
    "    converted_video_path = convert_to_mp4(video)\n",
    "    return converted_video_path\n",
    "\n",
    "def interpolate(choice, video=None, img1=None, img1_1=None, img2=None, img2_1=None, exp_num_frames=None):\n",
    "    if choice == \"Upload a video\":\n",
    "        processed_video, frames, average_l1_loss = process_video(video, exp_num_frames)\n",
    "        return processed_video, None, frames, average_l1_loss  # Video output, no images, and frames gallery\n",
    "    elif choice == \"Upload two images\":\n",
    "        if img1 is not None and img2 is not None:\n",
    "            generated_frames, average_l1_loss = process_images(img1, img1_1, img2, img2_1, exp_num_frames)  # Placeholder for interpolated frames\n",
    "            return None, generated_frames, None, average_l1_loss  # No video, only images, no frames gallery\n",
    "        else:\n",
    "            return None, None, None, None\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h1 style='text-align: center;'>Missing Frames Interpolation</h1>\")\n",
    "    choice = gr.Radio([\"Upload a video\", \"Upload two images\"], label=\"Choose Input Type\")\n",
    "\n",
    "\n",
    "    num_frames = gr.Slider(1, 10, value=2, step=1, label=\"Number of Interpolated Frames (2^n-1)\")\n",
    "\n",
    "    # Video input and processing\n",
    "    with gr.Row() as video_section:\n",
    "        video_input = gr.Video(label=\"Upload Video\", interactive=True)\n",
    "        video_output = gr.Video(label=\"Processed Video\", interactive=True)\n",
    "        frame_gallery = gr.Gallery(label=\"Processed Video Frames\")\n",
    "\n",
    "        \n",
    "    # Image input and processing\n",
    "    with gr.Row() as image_section:\n",
    "        img1_input = gr.Image(label=\"Upload 1st Image\", type=\"pil\")\n",
    "        img1_1_input = gr.Image(label=\"Upload 2nd Image\", type=\"pil\")\n",
    "        img2_input = gr.Image(label=\"Upload 3rd Image\", type=\"pil\")\n",
    "        img2_1_input = gr.Image(label=\"Upload 4th Image\", type=\"pil\")\n",
    "        # num_frames = gr.Slider(1, 5, value=2, step=1, label=\"Number of Interpolated Frames\")\n",
    "        image_output = gr.Gallery(label=\"Generated Frames\")\n",
    "\n",
    "\n",
    "    l1_loss_output = gr.Textbox(label=\"Average L1 Difference between adjacent frames\", interactive=False)\n",
    "\n",
    "    \n",
    "    # Initially hide both sections\n",
    "    video_section.visible = False\n",
    "    image_section.visible = False\n",
    "    l1_loss_output.visible = False\n",
    "    \n",
    "     # Toggle input visibility based on choice\n",
    "    def toggle_inputs(choice):\n",
    "        if choice == \"Upload a video\":\n",
    "            return gr.update(visible=True), gr.update(visible=False), gr.update(visible=True), gr.update(visible=True), gr.update(visible=True)\n",
    "        elif choice == \"Upload two images\":\n",
    "            return gr.update(visible=False), gr.update(visible=True), gr.update(visible=False), gr.update(visible=False), gr.update(visible=True)\n",
    "    \n",
    "    choice.change(toggle_inputs, inputs=[choice], outputs=[video_section, image_section, video_output, frame_gallery, l1_loss_output])\n",
    "\n",
    "    # Handle video upload to convert and display\n",
    "    video_input.upload(convert_to_mp4, inputs=video_input, outputs=video_input)\n",
    "\n",
    "    # Processing button\n",
    "    process_button = gr.Button(\"Process\")\n",
    "\n",
    "    # Ensure `interpolate` always returns outputs for video, gallery, and frames\n",
    "    process_button.click(\n",
    "        interpolate,\n",
    "        inputs=[choice, video_input, img1_input, img1_1_input, img2_input, img2_1_input, num_frames],\n",
    "        outputs=[video_output, image_output, frame_gallery, l1_loss_output]\n",
    "    )\n",
    "\n",
    "    gr.Markdown(\"### Instructions\")\n",
    "    gr.Markdown(\"\"\"\n",
    "        1. **Select** \"Upload a video\" to process an entire video offline or \"Upload two images\" to interpolate images.\n",
    "        2. **Adjust Settings** (e.g., number of frames) if available.\n",
    "        3. **Click Process** to view the interpolated video or frames. \n",
    "        \"\"\")\n",
    "    \n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
